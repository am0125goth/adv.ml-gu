{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73afd409-436f-4672-8bab-b068ecdc084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62c7e5d-4159-4869-ae65-5b0e517f24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a017011-6ec9-4e80-877e-556ad5e193b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datapath = \"/srv/data/lt2326-h25/a1/images\"\n",
    "info_datapath = \"/srv/data/lt2326-h25/a1/info.json\"\n",
    "ann_datapath = \"/srv/data/lt2326-h25/a1/train.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b6ace-cb88-4ade-be45-636b8b0867f0",
   "metadata": {},
   "source": [
    "Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76bc175-25a0-48a7-83a6-4357db0f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageSegmentation(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, info_file, img_size=256):\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.names = []\n",
    "        with open(info_file, \"r\", encoding=\"utf-8\") as a:\n",
    "            info = json.load(a)\n",
    "            for item in info[\"train\"]:\n",
    "                self.names.append(item[\"file_name\"])\n",
    "\n",
    "        self.img_files = {}\n",
    "        for f in os.listdir(img_dir):\n",
    "            name = os.path.basename(f)\n",
    "            if name in self.names:\n",
    "                self.img_files[name] = os.path.join(img_dir, f)\n",
    "                \n",
    "        self.samples = []\n",
    "        with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                if obj[\"file_name\"] in self.img_files:\n",
    "                    self.samples.append(obj)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        file_name = sample[\"file_name\"]\n",
    "        img_path = self.img_files[file_name]\n",
    "\n",
    "        #load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = image.size\n",
    "        image = image.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = transforms.functional.normalize(image, mean=[0.5]*3, std=[0.5]*3)\n",
    "\n",
    "        #create blank binary mask\n",
    "        bi_mask = Image.new(\"L\", (w, h), 0)\n",
    "        draw = ImageDraw.Draw(bi_mask)\n",
    "\n",
    "        # draw polygons from ann\n",
    "        for ann_group in sample.get(\"annotations\", []):\n",
    "            for ann in ann_group:\n",
    "                if isinstance(ann, dict):\n",
    "                    if ann.get(\"is_chinese\") and not ann.get(\"ignore\"):\n",
    "                        polygon = [tuple(p) for p in ann[\"polygon\"]]\n",
    "                        draw.polygon(polygon, outline=1, fill=1)\n",
    "\n",
    "        bi_mask = bi_mask.resize((self.img_size, self.img_size), Image.NEAREST)\n",
    "        bi_mask = torch.from_numpy(np.array(bi_mask)).long()\n",
    "        bi_mask = bi_mask.unsqueeze(0)\n",
    "        return image, bi_mask\n",
    "        \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba50a08-f4ac-4a81-81b9-3cf3ff9613bf",
   "metadata": {},
   "source": [
    "Dataset split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db22f35a-1f12-4909-b5a8-75ec475bdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=26):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "\n",
    "    tot_len = len(dataset)\n",
    "    train_len = int(tot_len*train_ratio)\n",
    "    val_len = int(tot_len*val_ratio)\n",
    "    test_len = tot_len - train_len - val_len\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    return random_split(dataset, [train_len, val_len, test_len], generator=generator)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91242a32-59ac-48ab-8433-7118f39481bf",
   "metadata": {},
   "source": [
    "Create Dataset and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f053c2f0-ca4a-4d60-93e2-d83f75732988",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BinaryImageSegmentation(\n",
    "    img_dir=img_datapath,\n",
    "    ann_file=ann_datapath,\n",
    "    info_file=info_datapath,\n",
    "    img_size=256\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ddbc7-d954-40bf-b705-6a59120729ad",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d26b62-1eb5-4fb9-b4ae-af8fc84df832",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100e687-05e3-4eb7-92f8-44cefc5dce37",
   "metadata": {},
   "source": [
    "create training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0df0d9-d43f-4626-b2a1-c576f948fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, masks in dataloader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss /len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf1fbd8-d505-4e90-accf-b05b028f3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in dataloader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            gts.append(masks.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    gts = np.concatenate(gts, axis=0)\n",
    "    return total_loss/len(dataloader), preds, gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c4d709-fc38-4897-abd1-08222879f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, masks, smooth=1e-6):\n",
    "    preds = preds.contiguous().view(-1)\n",
    "    masks = masks.contiguous().view(-1)\n",
    "\n",
    "    intersection = (preds * masks).sum()\n",
    "    dice = (2. * intersection + smooth) / (preds.sum() + masks.sum() + smooth)\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f3f34e-2c71-4d61-8027-d4ed7a3f5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, masks):\n",
    "        return dice_loss(preds, masks, smooth=self.smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e886846-f078-4f2c-a2ea-4cd6b038ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, device, epochs=50):\n",
    "    from lab1_eval import (plot_training_curve, eval_threshold, iou, dice, eval_testset, visualize_pred)\n",
    "    \n",
    "    criterion = DiceLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_loss = epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_preds, val_masks = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {e+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss > best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    #eval after training\n",
    "    \n",
    "\n",
    "    \n",
    "    plot_training_curve(train_losses, val_losses)\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics = eval_threshold(val_masks, val_preds, thresholds)\n",
    "    for m in metrics:\n",
    "        print(f\"Threshold={m['threshold']:.2f} | Precision={m['precision']:.3f} | Recall={m['recall']:.3f} | F1={m['f1']:.3f} | Accuracy={m['accuracy']:.3f}\")\n",
    "\n",
    "    best_threshold = max(metrics, key=lambda x: x['f1'])['threshold']\n",
    "    print(f'Best threshold for validations set: {best_threshold}')\n",
    "    \n",
    "    _, test_preds, test_masks = validate(model, test_loader, criterion, device)\n",
    "    test_metrics = eval_testset(test_preds, test_masks, threshold=best_threshold)\n",
    "    print(f\"Final test metrics: {test_metrics}\")\n",
    "\n",
    "    for i in range(3):\n",
    "        img, mask = test_loader.dataset[i]\n",
    "        visualize_pred(img.permute(1, 2, 0), mask, test_preds[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7179b6f-81ec-41bb-822c-22d574550728",
   "metadata": {},
   "source": [
    "Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713cf582-94a6-4326-ac77-ee99068687e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUNet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PixelPredictModel_UNet \n\u001b[1;32m      2\u001b[0m UNet_model \u001b[38;5;241m=\u001b[39m PixelPredictModel_UNet(in_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUNet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, device, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 11\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     val_loss, val_preds, val_masks \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m     14\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m imgs, masks \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/UNet.py:58\u001b[0m, in \u001b[0;36mPixelPredictModel_UNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Decoder (with skip connections) block\u001b[39;00m\n\u001b[1;32m     57\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup4(b)\n\u001b[0;32m---> 58\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcat([d4, e4], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec4(d4)\n\u001b[1;32m     61\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(d4)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from UNet import PixelPredictModel_UNet \n",
    "UNet_model = PixelPredictModel_UNet(in_ch=3, out_ch=1).to(device)\n",
    "\n",
    "train_model(UNet_model, train_loader=train_loader,val_loader=val_loader, test_loader=test_loader, device=device, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5ba8b-2034-47d3-b7c9-e71457c8f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SegNet import PixelPredictModel_SegNet\n",
    "SegNet_model = PixelPredictModel_SegNet(in_ch=3, out_ch=1).to(device)\n",
    "\n",
    "train_model(SegNet_model, train_loader=train_loader,val_loader=val_loader, test_loader=test_loader, device=device, epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
